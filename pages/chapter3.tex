% Chapter 3

\chapter{提升回归树（BRTs）}
本文采用提升回归树（BRTs）算法进行数据模拟\cite{elithWorkingGuideBoosted2008}。

\section{确定函数形式$f(.)$}
对数据进行回归的方法有很多，可以按照是否提前预设回归形式分为参数回归方法和非参数回归方法。
\begin{itemize}

\item 参数回归会对函数形式$f(.)$进行假设，一般为线性模型。正是因为提前预设好了函数形式，所以只需对回归系数$\beta$进行拟合，比较容易实现；但缺点便是这种提前设定好的函数形式可能并非完全符合现实情况，如果数据背后真实的函数形式为非线性，则按照线性函数去回归，结果可信度极低，也不能有效进行数据预测。

\item 而非参数回归不会提前设定好函数形式$f(.)$，相反，这种方法将寻找一种最贴合数据的函数形式，正是这种灵活性让函数形式更为多样化。由于其自由度较高，所以更为拟合模型的真实函数关系，但这也带来了缺陷，便是需要大量的数据才可以对$f(.)$进行最为准确的预测，若是数据量太小、或是有离群点的影响，函数会对数据过拟合，数据预测效果大幅减弱。
\end{itemize}

在具体实证过程中，我们可以选用的方法有很多。传统的线性回归方法有最小二乘法线性回归（OLS）、Lasso回归（Lasso）、岭回归（Ridge）等，传统机器学习算法 包括支持向量机（Support Vector Machines，SVM）、梯度提升树（Gradient Boosting Decision Tree，GBDT）、随机森林（Random Forest，RF）等等，以及其他的深度学习算法。

可是，这些方法在灵活性和可解释性两个维度上不能兼得，一般而言参数模型比较受限，自由度不高，但是理解起来比较容易，因为有着具体的函数形式，例如Lasso回归、最小二乘法线性回归等方法；而非参数模型更为灵活，不会提供具体的函数形式，难以做到可视化，多为机器学习算法。

\section{回归树}
目前，最流行的两类机器学习算法莫过于神经网络算法（卷积神经网络、循环神经网络、生成式对抗网络和图神经网络）与树形算法（随机森林、GBDT、XGBoost和LightGBM）\cite{microstrongRegressionTreeHuiGuiShu2019}。树形算法的基础组成部分便是决策树，由于其易理解、易构建、速度快等特点，被广泛的应用在数据挖掘、机器学习等领域。

根据处理数据类型的不同，决策树又分为两类：分类决策树与回归决策树。分类决策树主要用于处理定性指标、离散型数据，回归决策树用于处理定量指标、连续型数据。

决策树的原理便是模仿树的生长过程，在向下生长的过程中产生各种内部结点（Internal Node），每个内部结点又分支结出叶结点（Leaf Node）。内部结点表示一个特征或属性，即根据该特征把观测值归类到不同分支；而叶结点表示一个类别或者某个值，主要指落入该组所有数据的预测值。
对于回归树而言，便是将所有数据$X_1, X_2, \ldots, X_p$分到$J$个空间上不重合的区域$R_1, R_2, \ldots, R_J$里面，对位于每个区域$R_j$的数据，其预测值即为$R_j$内所有观测值的均值。在进行决策过程时，根据输入样本每个特征维度值的大小，从上往下在每一结点进行选择和分支，最终落入$J$个区域中的一个。

理论上来说，分割的区域有着任意的形状，但在实际情况中，为了方便操作和理解，多采用高维矩形的分割形状，最终目标是找到能够使残差平方和$RSS$最小化的分割区域$R_1, R_2, \ldots, R_J$，$RSS$计算方法如下所示，其中$\hat{y}_{R_{j}}$代表第$j$个区域内所有观测点的平均值。
$$\sum_{j=1}^{J} \sum_{i \in R_{j}}\left(y_{i}-\hat{y}_{R_{j}}\right)^{2}$$

由于回归树向下分化的可能性太多，无法通过穷举的方法进行选择，所以其采用的方法是递归二元分裂（Recursive Binary Splitting），即从上往下逐步分割，每个结点分到两个方向，对于任意$j$和$s$，进行如下分割：
$$
R_{1}(j, s)=\left\{X \mid X_{j}<s\right\} \quad R_{2}(j, s)=\left\{X \mid X_{j} \geq s\right\}
$$

在任意结点上，通过选择最优的系数$j$和$s$，使得两组$RSS$最小化。其中$\hat{y}_{R_{1}}$ 是组$R_{1}(j, s)$所有观测点的平均值，$\hat{y}_{R_{2}}$是组$R_{2}(j, s)$的平均值。
$$
RSS = \sum_{i: x_{i} \in R_{1}(j, s)}\left(y_{i}-\hat{y}_{R_{1}}\right)^{2}+\sum_{i: x_{i} \in R_{2}(j, s)}\left(y_{i}-\hat{y}_{R_{2}}\right)^{2}
$$

按照以上方法确定好了数据空间的切割方式，即生成完整回归树后，对于任意输入数据样本$x$，其输出值如下式所示。其中，当$x\in R_j$时，$I(x\in R_j)$取1；$x\notin R_j$时，$I(x\in R_j)$取0。
$$
f(x) = \sum_{j=1}^{J} \hat{c}_j I(x\in R_j)
$$
%
%但是，回归树不可能做到无限向下分裂，如果树的形状过于复杂，每组仅有极少量的数据时，可能会造成数据的过拟合，预测效果也不佳。一般来说，树终止分裂有以下几个条件：
%\begin{itemize}
%\item 当某一结点中所有观测点的值相同，自然没有必要继续分裂，将直接返回相同的值。
%\item 树的深度达到了预先设定的最大值，或是树叶数量达到预先设定的值等等。
%\item 不纯度（Impurity）的减小量小于预先设定好的阈值，即进一步的数据分割并不能显著降低数据不纯度的时候便停止分裂了。
%\item 结点的数据量小于预先定好的阈值。
%\end{itemize}
%
%\subsection{剪枝}
%虽然决策树会自动停止向下继续分裂的过程，但在每个内部结点上递归二元分裂是“贪婪”（Greedy）的，仅仅考虑了在该结点上分割的有效性，但未考虑是否会影响后面的数据分割效果，所以有时候也会造成过拟合的情况。所以在树模型中，我们会人为进行树的剪枝（Pruning），主要包括预剪枝 (Pre-Pruning)和后剪枝 (Post-Pruning)。
%\begin{itemize}
%\item 预剪枝的核心思想就是，在对结点进一步分割之前，预先采用验证集的数据去验证如此划分是否能真正提高划分的准确性，提升的准确性是否大于提前设定好的阈值。如果不能，就把结点标记为叶结点并退出进一步划分；如果可以就继续递归生成结点。
%\item 后剪枝则是不预先进行树形状的限制，任其分割生长。从训练集生成一颗完整的决策树后，自下向上地对内部结点进行考察，若把该结点以下的叶结点砍掉，即将该内部结点变为叶结点后，能够带来泛化性能的提升，则将对此结点进行剪枝。
%后剪枝的算法有很多，包括错误率降低剪枝（Reduced-Error Pruning，REP）、悲观剪枝（Pessimistic Error Pruning，PEP）、代价复杂度剪枝（Cost-Complexity Pruning，CCP）等。
%\end{itemize}


\section{Boosting提升方法}
回归树的优点有很多：便于理解、类似于人的思维方式、数据可视化等等，但缺点便是相比于其他的机器学习算法，其预测准确度仍然较低。
所以在处理实际问题时，只用单一的回归树是不够的，可以用套袋法（Bagging）、随机森林（Random Forest）以及提升方法（Boosting）进行模型的改进。
本文将利用集成学习中的Boosting框架，得到的新模型便是提升回归树（Boosting Regression Tree）。

在概率近似正确（Probably Approximately Correct，PAC）学习的框架下，如果一个概念或一个类，存在一个多项式的算法能够学习它，并且正确率很高，那么就称其为强可学习（Strongly Learnable）的；相反，如果一个概念存在一个多项式的学习算法能够学习它，但是学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习（Weakly Learnable）的\cite{microstrongShenRuLiJieTiShengShuBoostingTree2019}。在PAC的学习框架下，一定能够通过组合弱学习器来得到一个强学习器。

Boosting以一种高度自适应的方法顺序地学习这些弱学习器，其中每个基础模型都依赖于前面的模型，最后按照某种确定性的策略将它们组合起来，以提高分类的性能。
即先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，然后基于调整后的样本集训练下一个基学习器，如此反复，直到学习得到的基学习器的达到设定的个数，最终根据这些基学习器的预测表现去施加不同的权重，结合成强学习器\cite{JiQiXueXiBiJiGBDTYuanLiGradientBoosting}。
其背后思想便是综合多个方法的预测结果，让结果更有效。

在回归树模型中，实际采用加法模型（Additive Model）与前向分步算法（Forward Stagewise Algorithm）进行提升。

首先采用前向分步算法确定最优参数，确定初始提升树为$f_{0}(x)=0$，第$m$步的模型为：
$$
f_{m}(x)=f_{m-1}(x)+T\left(x;\Theta_{m}\right)
$$

其中，$f_{m-1}(x)$为当前模型，第$m$棵决策树参数$\Theta_{m}$将通过最小化损失函数$L$来确定：
$$
\hat{\Theta}_{m}=\operatorname{argmin}_{\left(\Theta_{m}\right)} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \Theta_{m}\right)\right)
$$

当损失函数$L$采用平方误差时，即$L(y, f(x))=(y-f(x))^{2}$，则其损失将变为下式，其中$r=y-f_{m-1}(x)$是当前模型拟合数据时的残差。
$$
L\left(y, f_{m-1}(x)+T\left(x ; \Theta_{m}\right)\right)=\left[y-f_{m-1}(x)-T\left(x ; \Theta_{m}\right)\right]^{2}=\left[r-T\left(x ; \Theta_{m}\right)\right]^{2}
$$

最终的提升树可以表示为决策树的加法模型。其中, $T\left(x ; \Theta_{m}\right)$ 表示决策树、$\Theta_{m}$为决策树的参数、$M$为树的个数。
$$
f_{M}(x)=\sum_{m=1}^{M} T\left(x ; \Theta_{m}\right)
$$


\section{梯度提升决策树（GBDT）}
在上节中介绍了提升树的方法，其利用加法模型和前向分步算法实现学习的优化过程。当损失函数为平方损失和指数损失函数时，每一步的优化很容易实现。但对于一般的损失函数而言，优化过程却没有那么简单。为了解决这一问题，\citeauthor{friedmanGreedyFunctionApproximation2001}（\citeyear{friedmanGreedyFunctionApproximation2001}）提出了梯度提升算法（Gradient Boost），其利用最速下降法（Steepest Descent），每一次建立模型是在之前模型损失函数的梯度下降方向，使得残差在梯度方向上减少，即利用损失函数的负梯度作为提升树算法残差的近似值，去拟合回归树，这便是提升回归树BRTs的一种改进算法——梯度提升决策树（Gradient Boosting Decision Tree，GBDT）\cite{GBDTTiDuTiShengJueCeShuJianShu,JiQiXueXiBiJiGBDTYuanLiGradientBoosting}。

GBDT是机器学习中一个长盛不衰的算法模型，具有训练效果好、不易过拟合等优点，其不仅在工业界应用广泛，也被多用于分类、点击率预测、搜索排序等任务，在各类数据挖掘竞赛中也有着不俗表现，据统计，Kaggle上的比赛有一半以上的冠军方案都是基于GBDT去实现的\cite{microstrongShenRuLiJieLightGBM2020}。

由于GBDT是在BRTs基础上进行改进的，仅在损失函数的优化上有所变化，所以在算法上大同小异。首先确定初始决策树：
$$f_{0}(x)=\arg \min _{\gamma} \sum_{i=1}^{N} L\left(y_{i}, \gamma\right)$$

对于从$m=1, 2, \ldots, M$的所有决策树，以及每个样本$i=1, 2, \ldots N$，其损失函数的负梯度为：
$$
r_{i m}=-\left[\frac{\partial L\left(y_{i}, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right]_{f=f_{m-1}}
$$

进而根据算出来的残差$r_{i m}$去拟合一个对应叶子结点为$R_{j m}$的决策树，其中$j=1,2, \ldots, J_{m}$。对于叶结点$j=1,2, \ldots, J_{m}$，计算出最佳的拟合值：
$$
\gamma_{j m}=\arg \min _{\gamma} \sum_{x_{i} \in R_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma\right)
$$

以上步骤相当于回归树遍历所有切分变量$j$和切分点$s$以找到最优的$j$和$s$，然后在每个结点区域求最优的$\gamma$。

更新第$m$个决策树为：
$$f_{m}(x)=f_{m-1}(x)+\sum_{j=1}^{J_{m}} \gamma_{j m} I\left(x \in R_{j m}\right)$$

最后得到回归树：
$$
\hat{f}(x)=f_{M}(x)=\sum_{m=1}^{M} f_{m}(x)=\sum_{m=1}^{M} \sum_{j=1}^{J} c_{m j} I\left(x \in R_{m j}\right)
$$

\section{LightGBM算法}
目前实现GBDT模型的算法主要有四种：scikit-learn、XGBoost、LightGBM和CatBoost。
但在算法原理方面，四种方法各有千秋，后三种方法都在最基础的GradientBoostingRegressor函数上进行优化。
\begin{itemize}
\item XGBoost对所有特征进行预排序，并在遍历分割点的时候用$O(\#data)$的代价找到特征上的最好分割点，最后将数据自分割点分裂成左右子结点。相比于GradientBoostingRegressor函数采用牛顿法、泰勒展开进行优化\cite{OptimizationXGBoostLoss}。
\item LightGBM是基于Histogram的决策树算法，采用leaf-wise策略分裂叶子结点，训练速度快、内存占用低、能够有效处理高维大数据。
\item CatBoost能够有效处理类别型特征，以对称树作为基学习器，减少了预测时间，也能够避免数据的过拟合。
\end{itemize}

由于本文数据维度高、体量大、且全为数值型数据，在保证结果准确的同时也要保持运行速度，所以将采用LightGBM算法进行文章后续的实证检验，为避免方法选择的不同造成结果差异，我们同样采用上述其他算法进行验证，发现不同方法结果类似，说明本文的实证结果与算法的选择无关。

对于LightGBM算法参数的选择，均采用网格调参（Grid Search）的方式。本文选取了对BRTs算法影响程度最大的参数：树的数量$n\_estimators$和学习率$learning\_rate$去进行调整，由于LightGBM算法采用leaf-wise策略进行结点的分裂，为了预防数据的过拟合问题，所以也调整了树的深度$max\_depth$。

由于公司每期的内在价值只能由本期的财务指标进行预测，而非由过去的财务数据，故本文需要逐期进行模型参数的选择，于每期进行一次网格调参，得到预测准确率最高的模型最优参数后，进而预测该期的公司内在价值。

\section{机器学习方法的局限性}
虽然机器学习能够对数据进行很好的非线性拟合，但从另一个方面来说，线性关系是能够被可视化的，也是容易被理解的，而机器学习却是一个“黑匣子”（Black Box），不会展现出具体的回归函数形式，只能做到输入值与输出值的一一对应，很难从逻辑上给出变量之间的联系\cite{zhaoJiQiXueXiZaiJinRongZiChanJieGeYuCeHePeiZhiZhongDeYingYongYanJiuShuPing2020}。
特别是在深度学习中，这个问题变得更加明显，例如在深度神经网络算法中，数据的抽象特征由神经网络经过多次计算而得，没有办法将原始数据和最终结果结合到一起。
而这一缺陷也会影响很多金融领域的继续深入，例如在资产定价问题上，我们能通过机器学习方法得到某些因子对于投资组合收益的显著预测作用，但很难再去检验为什么是这些因子，或者因子的具体影响途径等等。

另外，机器学习方法十分依赖于训练集数据\cite{luoJiYuWangLuoShuJuWaJueDeZiChanDingJieYanJiuShuPing2020}。虽然机器学习的优势在于能够对数据能够深度学习，并给出有效结果，但结果都是基于输入的数据，数据的质量好坏决定了机器学习预测的上限。而且在金融研究中常用市场的历史数据，而这样的数据一般很难有非常完美、对称的数据结构，或是缺失值太多等等，或多或少都有些局限性。

\section{机器学习方法的适用性}
在本文的研究中，当我们尝试根据财务指标构建错误定价因子时，会遇到高维度的横截面数据预测问题，而且对于回归变量之间的关系是线性还是非线性的，是否存在显着的相互影响作用，这些都不得而知，但我们可以通过使用BRTs克服这些限制。

首先，BRTs在包括金融在内的各个领域内均表现出强大的预测性能。其次，BRTs可以处理大型的高维数据集，它们可以自动进行变量选择和缩尾工作，对异常值具有鲁棒性。第三，BRTs不像其他机器学习方法那样是“黑匣子”，相反，该方法因为其可解释性高而闻名。

在实证过程中，可以看到采用BRTs方法在预测股票价格方面的出色表现，在后文的结果中，我们构建了相应的投资组合并取得了显著的正收益。此外，我们同样采用线性方法进行数据模拟，结果显示根据BRTs算法提供的投资组合收益高于标准线性回归模型。

另外，在现实问题中鲜有能够完全呈现线性的变量关系，而BRTs方法通过非参数方法估计股票特征与收益之间的关系，正是它们之间复杂的关系促使诸如BRTs之类的机器学习方法成为了优于传统统计方法的选择。

%\section{脚注}
%注释是对论文中特定名词或新名词的注解。注释可用页末注或篇末注的一种。选择页末注的应在注释与正文之间加细线分隔，线宽度为 1 点，线的长度不应超过纸张的三分之一宽度。同一页类列出多个注释的，应根据注释的先后顺序编排序号。字体为宋体5号，注释序号以“\circled{1}、\circled{2}”等数字形式标示在被注释词条的右上角。页末或篇末注释条目的序号应按照“\circled{1}、\circled{2}”等数字形式与被注释词条保持一致，脚注序号每面更新。示例：这里有个注释\footnote{我是解释注释的}。
%
%\section{引用文中小节}\label{sec:ref}
%如引用小节~\ref{sec:ref}
%
%\section{引用参考文献}
%这是一个参考文献引用的范例：“\cite{江泽民1989能源发展趋势及主要节能措施}提出……”。还可以引用多个文献：“\cite{kuhn2004man,江泽民2008新时期我国信息技术产业的发展,江泽民1989能源发展趋势及主要节能措施}提出……”。不同的引用方法：“\citet{江泽民1989能源发展趋势及主要节能措施}”“\citep{江泽民2008新时期我国信息技术产业的发展}”更多引用命令请参阅 natbib 文档或 biblatex 文档。\nocite{*}
%
%文献引用需要配合 BibTeX 使用，很多工具可以直接生成 BibTeX 文件（如 EndNote、NoteExpress、百度学术、谷歌学术等），此处不作介绍。
%
%\section{链接相关}
%模板使用了 hyperref 包处理相关链接，使用 \verb|\href| 可以生成超链接，默认不显示链接颜色。如果需要输出网址，可以使用 \verb|\url| 命令，示例：\url{https://github.com}。